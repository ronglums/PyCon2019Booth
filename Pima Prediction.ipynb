{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Diabetes\n",
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas is a dataframe library\n",
    "import matplotlib.pyplot as plt # matplotlib.pyplot plots data\n",
    "import numpy as np # numpy provides N-dim object support"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/pima-data.csv\") # load Pima data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(df, size=11):\n",
    "    \"\"\"\n",
    "    Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "    \n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        size: vertical and horizontal size of the plot\n",
    "        \n",
    "    Displays:\n",
    "        matrix of correlation between columns. Blue-cyan-yellow-red-darkred => less to more correlated\n",
    "                                               0------------------------->1\n",
    "                                               Expect a darkred line running from top to bottom right\n",
    "    \"\"\"\n",
    "    \n",
    "    corr = df.corr() # data frame correlation function\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr) # color code the rectangles by correlation value\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns) # draw x tick marks\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns) # draw y tick marks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['skin']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_map = {True:1, False:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diabetes'] = df['diabetes'].map(diabetes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting the data\n",
    "70% for training, 30% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']\n",
    "predicted_class_names = ['diabetes']\n",
    "\n",
    "x = df[feature_col_names].values # predictor feature columns (8 X m)\n",
    "y = df[predicted_class_names].values # predicted class (1=true, 0=false) column (1 X m)\n",
    "split_test_size = 0.30\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=split_test_size, random_state=42)\n",
    "# test_size = 0.3 is 30%, 42 is the answer to everything"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We check to ensure we have the desired 70% train, 30% test split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:0.2f}% in training set\".format((len(x_train)/len(df.index))*100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(x_test)/len(df.index))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-split Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impute with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Impute with mean all 0 readings\n",
    "fill_0 = Imputer(missing_values=0, strategy=\"mean\", axis=0)\n",
    "\n",
    "x_train = fill_0.fit_transform(x_train)\n",
    "x_test = fill_0.fit_transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Initial Algorithm = Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create Gaussian Naive Bayes model object and train it with the data\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_model.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "nb_predict_train = nb_model.predict(x_train)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accurary: {0:.4f}\".format(metrics.accuracy_score(y_train, nb_predict_train)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "nb_predict_test = nb_model.predict(x_test)\n",
    "\n",
    "# import the performance metrics library\n",
    "# from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accurary: {0:.4f}\".format(metrics.accuracy_score(y_test, nb_predict_test)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, nb_predict_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, nb_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrain =  Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42) # Create random forest object\n",
    "\n",
    "rf_model.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "rf_predict_train = rf_model.predict(x_train)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accurary: {0:.4f}\".format(metrics.accuracy_score(y_train, rf_predict_train)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the testing data\n",
    "rf_predict_test = rf_model.predict(x_test)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accurary: {0:.4f}\".format(metrics.accuracy_score(y_test, rf_predict_test)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, rf_predict_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, rf_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrain = Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lf_model = LogisticRegression(C=0.7, class_weight=\"balanced\", random_state=42)\n",
    "lf_model.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "lf_predict_train = lf_model.predict(x_train)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accurary: {0:.4f}\".format(metrics.accuracy_score(y_train, lf_predict_train)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "lf_predict_test = lf_model.predict(x_test)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accurary: {0:.4f}\".format(metrics.accuracy_score(y_test, lf_predict_test)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lf_predict_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lf_predict_test))\n",
    "plt.plot(metrics.confusion_matrix(y_test, lf_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_start = 0.1\n",
    "C_end = 5\n",
    "C_inc = 0.1\n",
    "\n",
    "C_values, recall_scores =[], []\n",
    "\n",
    "C_val = C_start\n",
    "best_recall_score = 0\n",
    "while(C_val < C_end):\n",
    "    C_values.append(C_val)\n",
    "    lr_model_loop = LogisticRegression(C=C_val, class_weight=\"balanced\", random_state=42)\n",
    "    lr_model_loop.fit(x_train, y_train.ravel())\n",
    "    lr_predict_loop_test=lr_model_loop.predict(x_test)\n",
    "    recall_score=metrics.recall_score(y_test, lr_predict_loop_test)\n",
    "    recall_scores.append(recall_score)\n",
    "    if(recall_score > best_recall_score):\n",
    "        best_recall_score = recall_score\n",
    "        best_lr_predict_test = lr_predict_loop_test\n",
    "        \n",
    "    C_val = C_val + C_inc\n",
    "    \n",
    "best_score_C_val = C_values[recall_scores.index(best_recall_score)]\n",
    "print(\"first max value of {0:.3f} occurred at C={1:.3f}\".format(best_recall_score, best_score_C_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrain with class_weight='balanced' and C=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(C=best_score_C_val, class_weight=\"balanced\", random_state=42)\n",
    "lr_model.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "lr_predict_test = lr_model.predict(x_test)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accurary: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lr_predict_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lr_predict_test))\n",
    "print(metrics.recall_score(y_test,lr_predict_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}